{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13191967,"sourceType":"datasetVersion","datasetId":8359895}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:12:45.825184Z","iopub.execute_input":"2025-09-27T20:12:45.825773Z","iopub.status.idle":"2025-09-27T20:12:45.833922Z","shell.execute_reply.started":"2025-09-27T20:12:45.825745Z","shell.execute_reply":"2025-09-27T20:12:45.833090Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/rolechat/zzx.jsonl\n/kaggle/input/rolechat/lz.jsonl\n/kaggle/input/rolechat/lbxx.jsonl\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# 环境依赖\n!pip uninstall -y transformers accelerate peft bitsandbytes datasets\n!pip install -U torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 \\\n  transformers==4.40.2 accelerate==0.28.0 bitsandbytes==0.43.1 \\\n  peft==0.9.0 datasets==2.19.0 sentencepiece==0.2.0 protobuf==3.20.*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:12:45.835104Z","iopub.execute_input":"2025-09-27T20:12:45.835400Z","iopub.status.idle":"2025-09-27T20:13:02.332738Z","shell.execute_reply.started":"2025-09-27T20:12:45.835372Z","shell.execute_reply":"2025-09-27T20:13:02.331769Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.40.2\nUninstalling transformers-4.40.2:\n  Successfully uninstalled transformers-4.40.2\nFound existing installation: accelerate 0.28.0\nUninstalling accelerate-0.28.0:\n  Successfully uninstalled accelerate-0.28.0\nFound existing installation: peft 0.9.0\nUninstalling peft-0.9.0:\n  Successfully uninstalled peft-0.9.0\nFound existing installation: bitsandbytes 0.43.1\nUninstalling bitsandbytes-0.43.1:\n  Successfully uninstalled bitsandbytes-0.43.1\nFound existing installation: datasets 2.19.0\nUninstalling datasets-2.19.0:\n  Successfully uninstalled datasets-2.19.0\nRequirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.11/dist-packages (2.1.0)\nRequirement already satisfied: torchvision==0.16.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\nRequirement already satisfied: torchaudio==2.1.0 in /usr/local/lib/python3.11/dist-packages (2.1.0)\nCollecting transformers==4.40.2\n  Using cached transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\nCollecting accelerate==0.28.0\n  Using cached accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\nCollecting bitsandbytes==0.43.1\n  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nCollecting peft==0.9.0\n  Using cached peft-0.9.0-py3-none-any.whl.metadata (13 kB)\nCollecting datasets==2.19.0\n  Using cached datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: protobuf==3.20.* in /usr/local/lib/python3.11/dist-packages (3.20.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.18.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.14.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2024.3.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2.18.1)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.105)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2.1.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (2.32.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (11.2.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.33.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2024.11.6)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.28.0) (7.0.0)\nRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (19.0.1)\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (0.7)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (3.12.13)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.5.82)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (1.20.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.16.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.16.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.16.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.16.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.16.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.16.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.0) (2025.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.16.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.16.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.16.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.16.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.16.0) (2024.2.0)\nUsing cached transformers-4.40.2-py3-none-any.whl (9.0 MB)\nUsing cached accelerate-0.28.0-py3-none-any.whl (290 kB)\nUsing cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\nUsing cached peft-0.9.0-py3-none-any.whl (190 kB)\nUsing cached datasets-2.19.0-py3-none-any.whl (542 kB)\nInstalling collected packages: transformers, accelerate, peft, datasets, bitsandbytes\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.28.0 bitsandbytes-0.43.1 datasets-2.19.0 peft-0.9.0 transformers-4.40.2\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# 显示配置\nimport torch, bitsandbytes as bnb\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"GPU:\", torch.cuda.get_device_name(0))\nprint(\"BitsAndBytes version:\", bnb.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:13:02.333981Z","iopub.execute_input":"2025-09-27T20:13:02.334308Z","iopub.status.idle":"2025-09-27T20:13:02.339883Z","shell.execute_reply.started":"2025-09-27T20:13:02.334274Z","shell.execute_reply":"2025-09-27T20:13:02.338931Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nGPU: Tesla T4\nBitsAndBytes version: 0.43.1\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# 库函数引用\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel, LoraConfig, get_peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:13:02.341755Z","iopub.execute_input":"2025-09-27T20:13:02.342312Z","iopub.status.idle":"2025-09-27T20:13:02.358206Z","shell.execute_reply.started":"2025-09-27T20:13:02.342294Z","shell.execute_reply":"2025-09-27T20:13:02.357598Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import json\n\n# 数据集检查\ndef preview_head_tail(data_path: str, head: int = 5, tail: int = 5):\n    \"\"\"\n    预览对话数据，仅输出开头 head 条和结尾 tail 条\n    Args:\n        data_path (str): jsonl 文件路径\n        head (int): 开头\n        tail (int): 结尾\n    \"\"\"\n    conversations = []\n    \n    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                conv = json.loads(line)[\"conversations\"]\n            except Exception as e:\n                print(\"跳过出错行:\", line[:100], e)\n                continue\n\n            # 提取 system/user/assistant 的对话内容\n            parts = [msg[\"content\"] for msg in conv if msg[\"role\"] in [\"system\", \"user\", \"assistant\"]]\n            conversations.append(\" / \".join(parts))\n    \n    total = len(conversations)\n    print(f\"{data_path}： {total} 条数据\")\n\n    # 输出开头 head 条\n    print(\"\\n=== 开头 ===\")\n    for c in conversations[:head]:\n        print(c)\n\n    # 输出结尾tail 条\n    print(\"\\n=== 结尾 ===\")\n    for c in conversations[-tail:]:\n        print(c)\n\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:13:02.359005Z","iopub.execute_input":"2025-09-27T20:13:02.359216Z","iopub.status.idle":"2025-09-27T20:13:02.375038Z","shell.execute_reply.started":"2025-09-27T20:13:02.359201Z","shell.execute_reply":"2025-09-27T20:13:02.374353Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import json\n\n# 读取数据集\ndef load_conversations(data_path: str):\n    \"\"\"\n    数据集格式：{\"messages\": [{\"role\": \"user\", \"content\": \"句子1\"}, {\"role\": \"assistant\", \"content\": \"句子2\"}]}\n    句子作为用户输入范例，句子2作为输出参考\n    从 jsonl 文件读取对话数据，转换为 (prompt, completion) 对\n    Args:\n        data_path (str): jsonl 文件路径\n    Returns:\n        list: [(prompt, completion), ...]\n    \"\"\"\n    pairs = []\n    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                conv = json.loads(line)[\"conversations\"]\n            except Exception as e:\n                print(\"跳过出错行:\", line[:100], e)\n                continue\n\n            persona = user_msg = assistant_msg = \"\"\n            for msg in conv:\n                if msg[\"role\"] == \"system\":\n                    persona = msg[\"content\"]\n                elif msg[\"role\"] == \"user\":\n                    user_msg = msg[\"content\"]\n                elif msg[\"role\"] == \"assistant\":\n                    assistant_msg = msg[\"content\"]\n\n            prompt = f\"{persona}\\n用户: {user_msg}\\n助手: \"\n            completion = assistant_msg\n            pairs.append((prompt, completion))\n\n    print(f\"{data_path}： {len(pairs)} 条数据可用\")\n    return pairs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:13:02.375873Z","iopub.execute_input":"2025-09-27T20:13:02.376082Z","iopub.status.idle":"2025-09-27T20:13:02.393881Z","shell.execute_reply.started":"2025-09-27T20:13:02.376065Z","shell.execute_reply":"2025-09-27T20:13:02.393092Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# 量化压缩，加速微调\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\n# 加载预训练大模型\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"THUDM/chatglm3-6b\",\n    trust_remote_code=True,\n    device_map={\"\": 0}, \n    quantization_config=bnb_config,\n    torch_dtype=torch.float16\n)\n\n# 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\n    \"THUDM/chatglm3-6b\",\n    trust_remote_code=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:13:02.394805Z","iopub.execute_input":"2025-09-27T20:13:02.395434Z","iopub.status.idle":"2025-09-27T20:13:27.266390Z","shell.execute_reply.started":"2025-09-27T20:13:02.395416Z","shell.execute_reply":"2025-09-27T20:13:27.265723Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b52d2318a964fbab259c33aeab3810f"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# 配置 LoRA\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nloaded_loras = dict()\n\n# ====== 角色对应的 LoRA ======\nrole2lora = {\n    \"蜡笔小新\": \"/kaggle/working/lora/lora_lbxx\",\n    \"老子\": \"/kaggle/working/lora/lora_lz\",\n    \"蜘蛛侠\": \"/kaggle/working/lora/lora_zzx\",\n    \"c4\": \"/kaggle/working/lora/lora_c4\",\n    \"c5\": \"/kaggle/working/lora/lora_c5\",\n    \"c6\": \"/kaggle/working/lora/lora_c6\",\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:25:19.089388Z","iopub.execute_input":"2025-09-27T20:25:19.089975Z","iopub.status.idle":"2025-09-27T20:25:19.095324Z","shell.execute_reply.started":"2025-09-27T20:25:19.089947Z","shell.execute_reply":"2025-09-27T20:25:19.094322Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# 模型微调\ndef train_lora(\n    data_path,\n    model_name=\"THUDM/chatglm3-6b\",\n    save_dir=\"/kaggle/working/lora\",\n    lr=1e-4,\n    max_steps=100,\n    lora=lora_config\n):\n    \"\"\"LoRA 微调主函数\"\"\"\n\n    # ========= 加载数据 =========\n    pairs = load_conversations(data_path)\n\n    # ========= # 配置 LoRA =========\n    lora_config = lora\n\n    model = get_peft_model(base_model, lora_config)\n    model.train()\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\n    # ========= 训练循环 =========\n    for step, (prompt, completion) in enumerate(pairs[:max_steps]):\n        full_text = prompt + completion\n        inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n\n        labels = inputs.input_ids.clone().to(model.device)\n\n        prompt_len = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].shape[-1]\n        labels[:, :prompt_len] = -100  # 避免计算 prompt 的 loss\n\n        outputs = model(**inputs, labels=labels)\n        loss = outputs.loss\n\n        if step % 5 == 0:\n            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    # ========= 保存模型 =========\n    os.makedirs(save_dir, exist_ok=True)\n    model.save_pretrained(save_dir)\n    tokenizer.save_pretrained(save_dir)\n    print(f\"LoRA 模型保存到 {save_dir}\")\n\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:13:27.274247Z","iopub.execute_input":"2025-09-27T20:13:27.274978Z","iopub.status.idle":"2025-09-27T20:13:27.290906Z","shell.execute_reply.started":"2025-09-27T20:13:27.274957Z","shell.execute_reply":"2025-09-27T20:13:27.290173Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# 蜡笔小新的lora\ndata_path = \"/kaggle/input/rolechat/lbxx.jsonl\"\ntrain_lora(data_path,\"THUDM/chatglm3-6b\",\"/kaggle/working/lora/lora_lbxx\",1e-4,100,lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:13:27.291559Z","iopub.execute_input":"2025-09-27T20:13:27.291820Z","iopub.status.idle":"2025-09-27T20:14:49.078051Z","shell.execute_reply.started":"2025-09-27T20:13:27.291762Z","shell.execute_reply":"2025-09-27T20:14:49.077370Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/rolechat/lbxx.jsonl： 110 条数据可用\n","output_type":"stream"},{"name":"stderr","text":"2025-09-27 20:13:30.528269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759004010.711870     299 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759004010.772256     299 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Step 0, Loss: 7.0273\nStep 5, Loss: 5.8203\nStep 10, Loss: 3.8320\nStep 15, Loss: 3.1836\nStep 20, Loss: 3.8477\nStep 25, Loss: 1.7988\nStep 30, Loss: 2.2363\nStep 35, Loss: 3.8711\nStep 40, Loss: 4.5781\nStep 45, Loss: 3.5156\nStep 50, Loss: 4.8359\nStep 55, Loss: 3.9023\nStep 60, Loss: 4.6367\nStep 65, Loss: 4.1602\nStep 70, Loss: 4.2266\nStep 75, Loss: 3.6758\nStep 80, Loss: 3.5605\nStep 85, Loss: 3.5723\nStep 90, Loss: 2.4785\nStep 95, Loss: 3.0137\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"LoRA 模型保存到 /kaggle/working/lora/lora_lbxx\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# 老子的lora\ndata_path = \"/kaggle/input/rolechat/lz.jsonl\"\ntrain_lora(data_path,\"THUDM/chatglm3-6b\",\"/kaggle/working/lora/lora_lz\",1e-4,100,lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:14:49.079972Z","iopub.execute_input":"2025-09-27T20:14:49.080456Z","iopub.status.idle":"2025-09-27T20:15:34.270746Z","shell.execute_reply.started":"2025-09-27T20:14:49.080434Z","shell.execute_reply":"2025-09-27T20:15:34.269904Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/rolechat/lz.jsonl： 110 条数据可用\nStep 0, Loss: 6.2539\nStep 5, Loss: 4.1602\nStep 10, Loss: 1.9600\nStep 15, Loss: 3.2480\nStep 20, Loss: 0.2035\nStep 25, Loss: 0.1285\nStep 30, Loss: 1.0645\nStep 35, Loss: 4.2422\nStep 40, Loss: 4.2109\nStep 45, Loss: 2.6348\nStep 50, Loss: 2.2578\nStep 55, Loss: 1.1748\nStep 60, Loss: 3.2559\nStep 65, Loss: 0.1835\nStep 70, Loss: 1.3584\nStep 75, Loss: 4.4727\nStep 80, Loss: 4.0273\nStep 85, Loss: 3.6230\nStep 90, Loss: 4.6484\nStep 95, Loss: 4.5430\nLoRA 模型保存到 /kaggle/working/lora/lora_lz\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# 蜘蛛侠的lora\ndata_path = \"/kaggle/input/rolechat/zzx.jsonl\"\ntrain_lora(data_path,\"THUDM/chatglm3-6b\",\"/kaggle/working/lora/lora_zzx\",1e-4,100,lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:15:34.272257Z","iopub.execute_input":"2025-09-27T20:15:34.272478Z","iopub.status.idle":"2025-09-27T20:16:18.070114Z","shell.execute_reply.started":"2025-09-27T20:15:34.272459Z","shell.execute_reply":"2025-09-27T20:16:18.069264Z"}},"outputs":[{"name":"stdout","text":"跳过出错行: {\"conversations\":[{\"role\":\"system\",\"content\":\"你是蜘蛛侠（守护纽约）\"},{\"role\":\"user\",\"content\":\"你觉得城市需要你吗？\"},{ Expecting ',' delimiter: line 1 column 119 (char 118)\n/kaggle/input/rolechat/zzx.jsonl： 109 条数据可用\nStep 0, Loss: 2.4355\nStep 5, Loss: 2.6289\nStep 10, Loss: 4.5547\nStep 15, Loss: 5.8984\nStep 20, Loss: 5.0742\nStep 25, Loss: 5.9883\nStep 30, Loss: 5.4688\nStep 35, Loss: 4.1641\nStep 40, Loss: 3.5234\nStep 45, Loss: 2.2012\nStep 50, Loss: 4.1211\nStep 55, Loss: 4.8711\nStep 60, Loss: 2.8711\nStep 65, Loss: 2.9238\nStep 70, Loss: 5.6133\nStep 75, Loss: 3.5430\nStep 80, Loss: 3.2852\nStep 85, Loss: 3.5234\nStep 90, Loss: 3.6387\nStep 95, Loss: 4.1914\nLoRA 模型保存到 /kaggle/working/lora/lora_zzx\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def load_lora(base_model, lora_path):\n    \"\"\"\n    给基座模型加载指定的 LoRA\n    \"\"\"\n    model = PeftModel.from_pretrained(base_model, lora_path).eval()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:16:18.071849Z","iopub.execute_input":"2025-09-27T20:16:18.072355Z","iopub.status.idle":"2025-09-27T20:16:18.077140Z","shell.execute_reply.started":"2025-09-27T20:16:18.072333Z","shell.execute_reply":"2025-09-27T20:16:18.076524Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# 回复生成函数\ndef generate_reply(role: str, user_input: str) -> str:\n    import re\n    \n    \"\"\"\n    输入角色（c1-c6）和用户输入，返回模型回复\n    \"\"\"\n    if role not in role2lora:\n        raise ValueError(f\"未知角色 {role}，可选: {list(role2lora.keys())}\")\n\n    # 如果没加载过该角色的 LoRA，就先加载\n    if role not in loaded_loras:\n        lora_path = role2lora[role]\n        print(f\"[INFO] 正在加载 LoRA: {role} ({lora_path})\")\n        loaded_loras[role] = PeftModel.from_pretrained(base_model, lora_path).eval()\n\n    model = loaded_loras[role]\n\n    # 构造 prompt\n    persona = f\"{role}的身份\"\n    prompt = f\"{persona}\\n用户: {user_input.strip()}\\n{role}: \"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    # 推理\n    outputs = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n\n    raw = tokenizer.decode(\n        outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True\n    ).strip()\n\n    # 基础清洗\n    answer = re.sub(r\"[\\r\\n]+\", \"\\n\", raw)\n\n    # 只提取角色回答，并去掉后续可能生成的 “用户:” 部分\n    match = re.search(rf\"{role}[:：]\\s*(.+)\", answer, re.DOTALL)\n    if match:\n        answer_clean = match.group(1).strip()\n        # 截断在第一个 “用户:” 之前\n        if \"用户:\" in answer_clean:\n            answer_clean = answer_clean.split(\"用户:\")[0].strip()\n    else:\n        answer_clean = answer.strip()\n\n    print(f\"用户: {user_input}\")\n    print(f\"{role}: {answer_clean}\")\n    print(\"-\" * 60)\n\n    return answer_clean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:16:18.078196Z","iopub.execute_input":"2025-09-27T20:16:18.078528Z","iopub.status.idle":"2025-09-27T20:16:18.097342Z","shell.execute_reply.started":"2025-09-27T20:16:18.078504Z","shell.execute_reply":"2025-09-27T20:16:18.096548Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"generate_reply(\"蜡笔小新\", \"你是谁？\")\ngenerate_reply(\"蜡笔小新\", \"你喜欢动感超人吗？\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:16:18.098542Z","iopub.execute_input":"2025-09-27T20:16:18.098873Z","iopub.status.idle":"2025-09-27T20:16:34.287245Z","shell.execute_reply.started":"2025-09-27T20:16:18.098847Z","shell.execute_reply":"2025-09-27T20:16:34.286582Z"}},"outputs":[{"name":"stdout","text":"[INFO] 正在加载 LoRA: 蜡笔小新 (/kaggle/working/lora/lora_lbxx)\n用户: 你是谁？\n蜡笔小新: 我是野原新之助，不过大家都叫我是小新。\n------------------------------------------------------------\n用户: 你喜欢动感超人吗？\n蜡笔小新: 喜欢！但我觉得他的红斗篷太重了。\n------------------------------------------------------------\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'喜欢！但我觉得他的红斗篷太重了。'"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"generate_reply(\"老子\", \"早睡早起怎么样？\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:16:34.288033Z","iopub.execute_input":"2025-09-27T20:16:34.288340Z","iopub.status.idle":"2025-09-27T20:16:38.768618Z","shell.execute_reply.started":"2025-09-27T20:16:34.288313Z","shell.execute_reply":"2025-09-27T20:16:38.767888Z"}},"outputs":[{"name":"stdout","text":"[INFO] 正在加载 LoRA: 老子 (/kaggle/working/lora/lora_lz)\n用户: 早睡早起怎么样？\n老子: 少私寡欲，自然规律。\n------------------------------------------------------------\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'少私寡欲，自然规律。'"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"generate_reply(\"蜘蛛侠\", \"你是谁？\")\ngenerate_reply(\"蜘蛛侠\", \"你住在哪？\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T20:25:23.290975Z","iopub.execute_input":"2025-09-27T20:25:23.291275Z","iopub.status.idle":"2025-09-27T20:25:49.658601Z","shell.execute_reply.started":"2025-09-27T20:25:23.291251Z","shell.execute_reply":"2025-09-27T20:25:49.657932Z"}},"outputs":[{"name":"stdout","text":"[INFO] 正在加载 LoRA: 蜘蛛侠 (/kaggle/working/lora/lora_zzx)\n用户: 你是谁？\n蜘蛛侠: 我是托尼，一个普通的高中生，被选中拥有特殊能力，为了保护城市，我会利用这些力量。\n------------------------------------------------------------\n用户: 你住在哪？\n蜘蛛侠: 纽约市。\n------------------------------------------------------------\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'纽约市。'"},"metadata":{}}],"execution_count":29}]}